{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset and preprocessing to get it in the desired shape\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Scaling the values to scale the data down\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "\n",
    "temp = X_train.reshape(60000,28*28).T\n",
    "temp = temp.reshape(28,28,1,60000)\n",
    "temp = temp[:,:,:,:1000]\n",
    "ytemp = y_train[:1000]\n",
    "\n",
    "temp1 = X_test.reshape(10000,28*28).T\n",
    "temp1 = temp1.reshape(28,28,1,10000)\n",
    "temp1 = temp1[:,:,:,:1000]\n",
    "ytemp1 = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten function is used to convert a convoluted matrix into a single array\n",
    "# to apply in FC layer\n",
    "def flatten(X):\n",
    "    shape_x=X.shape[0]\n",
    "    shape_y=X.shape[1]\n",
    "    shape_z=X.shape[2]\n",
    "    m=X.shape[3]\n",
    "    flattened_vec = X.reshape(shape_x*shape_y*shape_z,m)\n",
    "    return flattened_vec,shape_x,shape_y,shape_z,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-pad function is to do padding of the input matrix to minimize data loss\n",
    "def zero_pad(X, pad):\n",
    "    \n",
    "    X_pad = np.pad(X, ((pad,pad),(pad,pad),(0,0),(0,0)), 'constant', constant_values=0)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function is used in the final step\n",
    "def softmax(x):\n",
    "    softm = np.exp(x)/np.sum(np.exp(x),axis=1,keepdims=True)\n",
    "    return softm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu function is used as activation function in the convolution layers\n",
    "def relu(x):\n",
    "    y= np.maximum(0,x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient of relu function\n",
    "def relugradient(dact,act):\n",
    "    dhidden = dact\n",
    "    dhidden[act<=0]=0\n",
    "    return dhidden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "\n",
    "    N, D = x.shape\n",
    "    #step1: calculate mean\n",
    "    mu = 1./N * np.sum(x, axis = 0)\n",
    "    #step2: subtract mean vector of every trainings example\n",
    "    xmu = x - mu\n",
    "    #step3: following the lower branch - calculation denominator\n",
    "    sq = xmu ** 2\n",
    "    #step4: calculate variance\n",
    "    var = 1./N * np.sum(sq, axis = 0)\n",
    "    #step5: add eps for numerical stability, then sqrt\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "    #step6: invert sqrtwar\n",
    "    ivar = 1./sqrtvar\n",
    "    #step7: execute normalization\n",
    "    xhat = xmu * ivar\n",
    "    #step8: the two transformation steps\n",
    "    gammax = gamma * xhat\n",
    "    #step9\n",
    "    out = gammax + beta\n",
    "    #store intermediate\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbatchnorm_forward(X, gamma, beta, eps = 1e-5):\n",
    "    H, W, C, N = X.shape\n",
    "        # mini-batch mean\n",
    "    mu = np.mean(X, axis=(0, 1, 3))\n",
    "    xmu = X -mu.reshape(1,1,C,1)\n",
    "    sq = xmu**2\n",
    "        # mini-batch variance\n",
    "    var =(1/(H*W*N)) *np.sum(sq, axis=(0, 1, 3)).reshape((1,1,C,1))\n",
    "    sqrtvar = np.sqrt(var+eps)\n",
    "    ivar = 1./sqrtvar\n",
    "    xhat = xmu * ivar\n",
    "    #print(gamma.shape)\n",
    "    gammax = gamma.reshape(1,1,C,1) * xhat\n",
    "    out = gammax + beta.reshape(1,1,C,1)\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "    return out , cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "      #unfold the variables stored in cache\n",
    "    xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "      #get the dimensions of the input/output\n",
    "    N,D = dout.shape\n",
    "      #step9\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgammax = dout #not necessary, but more understandable\n",
    "      #step8\n",
    "    dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "    dxhat = dgammax * gamma\n",
    "  #step7\n",
    "    divar = np.sum(dxhat*xmu, axis=0)\n",
    "    dxmu1 = dxhat * ivar\n",
    "  #step6\n",
    "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "  #step5\n",
    "    dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "  #step4\n",
    "    dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "  #step3\n",
    "    dxmu2 = 2 * xmu * dsq\n",
    "  #step2\n",
    "    dx1 = (dxmu1 + dxmu2)\n",
    "    dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "  #step1\n",
    "    dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "  #step0\n",
    "    dx = dx1 + dx2\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbatchnorm_backward(dout, cache):\n",
    "    #unfold the variables stored in cache\n",
    "    xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "    #get the dimensions of the input/output\n",
    "    H,W,C,N = dout.shape\n",
    "    #step9\n",
    "    dbeta = np.sum(dout, axis=(0,1,3)).reshape(1,1,C,1)\n",
    "    dgammax = dout #not necessary, but more understandable\n",
    "    #step8\n",
    "    dgamma = np.sum(dgammax*xhat, axis=(0,1,3)).reshape(1,1,C,1)\n",
    "    dxhat = dgammax * gamma.reshape(1,1,C,1)\n",
    "    #step7\n",
    "    divar = np.sum(dxhat*xmu, axis=(0,1,3)).reshape(1,1,C,1)\n",
    "    dxmu1 = dxhat * ivar\n",
    "    #step6\n",
    "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "    #step5\n",
    "    dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "    #step4\n",
    "    dsq = (1. /(H*W*N)) * np.ones((H,W,C,N)) * dvar\n",
    "    #step3\n",
    "    dxmu2 = 2 * xmu * dsq\n",
    "    #step2\n",
    "    dx1 = (dxmu1 + dxmu2)\n",
    "    dmu = -1 * np.sum(dxmu1+dxmu2, axis=(0,1,3))\n",
    "    #step1\n",
    "    dx2 = (1. /(H*W*N)) * np.ones((H,W,C,N)) * dmu.reshape(1,1,C,1)\n",
    "    #step0\n",
    "    dx = dx1 + dx2\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_forward function is to apply convolution on the input using filters\n",
    "def conv_forward(X,W,padding=1,stride=1):       \n",
    "    p = int(padding)\n",
    "    if (padding is not 0):\n",
    "        X = zero_pad(X,p)\n",
    "        #X = np.pad(X, ((p,p),(p,p),(0,0),(0,0)), 'constant')\n",
    "    n = int(X.shape[0])\n",
    "    #print(n)\n",
    "    m = int(X.shape[3])\n",
    "   # print(m)\n",
    "    f = int(W.shape[0])\n",
    "    #print(f)\n",
    "    s = int(stride)\n",
    "    #print(s)\n",
    "    num_channels=int(X.shape[2])\n",
    "    num_filters=W.shape[3]\n",
    "    if (type((n-f)//s) is not int):\n",
    "        print((n-f)//s)\n",
    "        print('invalid padding or stride')\n",
    "        return\n",
    "    #output_size = int((n-f+2*p)/s) +1\n",
    "    output_size = int((n-f)/s) + 1\n",
    "    #print(output_size)\n",
    "    Z = np.zeros((output_size,output_size,num_filters,m))\n",
    "    for k in range(num_filters):\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                Z[i,j,k,:]=np.sum(X[i*s:(i*s+f),j*s:(j*s+f),:,:]*W[:,:,:,k].reshape(f,f,num_channels,-1),axis=(0,1,2))\n",
    "                \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_pool_forward(X,f=2,stride=2):       \n",
    "    n = int(X.shape[0])\n",
    "    m = int(X.shape[3])\n",
    "    s = int(stride)\n",
    "    num_filters=int(X.shape[2])\n",
    "    output_size = int((n-f)/s) +1\n",
    "    Z = np.zeros((output_size,output_size,num_filters,m))\n",
    "    for k in range(num_filters):\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                Z[i,j,k,:]=np.max(X[i*s:(i*s+f),j*s:(j*s+f),k,:],axis=(0,1))                \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ,W,X,padding=1,stride=1):\n",
    "   \n",
    " #    The backward computation for a convolution function\n",
    "#    \n",
    "#    Arguments:\n",
    "#    dZ -- gradient of the cost with respect to output of the conv layer (Z), numpy array of shape (n_H, n_W) \n",
    "#    W -- weights of the conv layer\n",
    "#    X -- input of the conv layer\n",
    "#    \n",
    "#    Returns:\n",
    "#    dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) \n",
    "#    dfilter -- gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f,f) \n",
    "\n",
    "    # Retrieving dimensions from W's shape\n",
    "    (f, f, num_channels,num_filters) = W.shape\n",
    "    \n",
    "    # Retrieving dimensions from dZ's shape\n",
    "    (n_H, n_W,_,_) = dZ.shape\n",
    "    \n",
    "    s = stride\n",
    "    pad = padding\n",
    "    # Initializing dX, dW with the correct shapes\n",
    "    dfilter = np.zeros(W.shape)\n",
    "    dX = np.zeros(X.shape)\n",
    "    m = X.shape[3]\n",
    "    \n",
    "    # Pad X and dX\n",
    "    X_pad = zero_pad(X, pad)\n",
    "    dX_pad = zero_pad(dX, pad)\n",
    "    \n",
    "    \n",
    "    # Looping over vertical(h) and horizontal(w) axis of the output\n",
    "    for k in range(num_filters):\n",
    "        # To eliminate the looping over each training example,\n",
    "        # we create a 5-dim filter matrix where the filter matrix is repeated along the 5th dim\n",
    "        shapedfilter=np.tile(W[:,:,:,k].reshape(f,f,num_channels,1),(1,1,1,1,m))[0]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                dX_pad[h*s:h*s+f, w*s:w*s+f,:,:] += shapedfilter * dZ[h,w,k,:]\n",
    "                dfilter[:,:,:,k] += np.sum(X_pad[h*s:h*s+f, w*s:w*s+f,:,:] * dZ[h,w,k,:],axis=3)\n",
    "                \n",
    "                #print(X[h:h+f, w:w+f,:].shape)\n",
    "    dX = dX[pad:-pad, pad:-pad, :,:]\n",
    "    return dfilter,dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_pool_backward(dA_pooled,A_act,f=2,stride=2):\n",
    "    dA_act = np.zeros(A_act.shape)\n",
    "    s = stride\n",
    "    (n_H,n_W,n_C,_)=dA_pooled.shape\n",
    "    dA_pooled_reshape=np.repeat(np.repeat(dA_pooled,[f],axis=1),[f],axis=0)\n",
    "    for c in range(n_C):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                #print(h,w)\n",
    "                mask = (A_act[h*s:h*s+f,w*s:w*s+f,c,:]==np.max(A_act[h*s:h*s+f,w*s:w*s+f,c,:],axis=(0,1)))\n",
    "                #print(mask[:,:,0])\n",
    "                dA_act[h*s:h*s+f,w*s:w*s+f,c,:]=dA_pooled_reshape[h*s:h*s+f,w*s:w*s+f,c,:]*mask                   \n",
    "    return dA_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y,y_prob):\n",
    "    loss_train=0\n",
    "    m = y.shape[0]\n",
    "    for i in np.arange(m):\n",
    "        loss_train = loss_train+ (-np.log(y_prob[i,y[i]]))\n",
    "    loss_train = loss_train/m\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y,y_prob):\n",
    "    pred=pd.DataFrame(y_prob).idxmax(axis=1)\n",
    "    return np.mean(pred==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tell_size(temp,ytemp,myfilter1,myfilter2):\n",
    "    feat_map = conv_forward(temp,myfilter1,stride=1)\n",
    "    feat_map_act = relu(feat_map)\n",
    "    feat_map_pool = cnn_pool_forward(feat_map_act,f=2,stride=2)\n",
    "    feat_map2= conv_forward(feat_map_pool,myfilter2,stride=1)\n",
    "    feat_map2_act=relu(feat_map2)\n",
    "    flattened_vec,shape_x,shape_y,shape_z,m = flatten(feat_map2_act)\n",
    "    return flattened_vec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(temp,ytemp,parameters,fc1_neurons,num_class):\n",
    "        (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)=parameters\n",
    "        feat_map = conv_forward(temp,myfilter1)\n",
    "        feat_map_bn,conv1_cache = convbatchnorm_forward(feat_map,conv1gam,conv1bet)\n",
    "        feat_map_act = relu(feat_map_bn)\n",
    "        feat_map_pool = cnn_pool_forward(feat_map_act,f=2,stride=2)\n",
    "        feat_map2= conv_forward(feat_map_pool,myfilter2)\n",
    "        feat_map2_bn,conv2_cache = convbatchnorm_forward(feat_map2,conv2gam,conv2bet)\n",
    "        feat_map2_act=relu(feat_map2_bn)\n",
    "        flattened_vec,shape_x,shape_y,shape_z,m = flatten(feat_map2_act)\n",
    "        shape = (shape_x,shape_y,shape_z,m)\n",
    "        fc1 = np.dot(flattened_vec.T,W1)\n",
    "        fc1_bn,fc_cache=batchnorm_forward(fc1, gamma=gam, beta=bet, eps = 1e-5)\n",
    "        fc1_act = relu(fc1_bn)\n",
    "        output = np.dot(fc1_act,W2)+b2\n",
    "        prob   = softmax(output)\n",
    "        loss=calculate_loss(ytemp,prob)\n",
    "        accuracy=accuracy_score(ytemp,prob)\n",
    "        cache =(feat_map_act,feat_map_pool,feat_map2_act,flattened_vec,shape,\n",
    "                fc1_act,prob,myfilter1,myfilter2,W1,W2,conv2_cache,conv1_cache,fc_cache)\n",
    "        \n",
    "        return loss,accuracy,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate(temp,ytemp,cache):\n",
    "        (feat_map_act,feat_map_pool,feat_map2_act,flattened_vec,shape,\n",
    "                fc1_act,prob,myfilter1,myfilter2,W1,W2,conv2_cache,conv1_cache,fc_cache)=cache\n",
    "        shape_x,shape_y,shape_z,m=shape\n",
    "        doutput = np.copy(prob)\n",
    "        m = ytemp.shape[0]\n",
    "        for i in np.arange(m):\n",
    "            doutput[i,ytemp[i]] = doutput[i,ytemp[i]]-1\n",
    "        doutput = doutput/m\n",
    "        dW2 = np.dot(fc1_act.T,doutput)\n",
    "        db2 = np.sum(doutput, axis=0, keepdims=True)\n",
    "        dfc1_act = np.dot(doutput,W2.T)\n",
    "        dfc1_bn = relugradient(dfc1_act,fc1_act)\n",
    "        dfc1,dgam,dbet = batchnorm_backward(dfc1_bn, fc_cache)\n",
    "        dW1 = np.dot(flattened_vec,dfc1)\n",
    "        #db3 shape (1,outputsize)\n",
    "#        db = np.sum(dfc1, axis=0, keepdims=True)\n",
    "        dflattened_vec = np.dot(dfc1,W1.T).T\n",
    "        dfeat_map2_act=dflattened_vec.reshape(shape_x,shape_y,shape_z,m)\n",
    "        dfeat_map2_bn = relugradient(dfeat_map2_act,feat_map2_act)   \n",
    "        dfeat_map2,dconv2gam,dconv2bet = convbatchnorm_backward(dfeat_map2_bn,conv2_cache)\n",
    "        #print(dconv2gam.shape)\n",
    "        dfilt2,dfeat_map_pooled = conv_backward(dfeat_map2,myfilter2,feat_map_pool)\n",
    "        dfeat_map_act=cnn_pool_backward(dfeat_map_pooled,feat_map_act,f=2,stride=2)\n",
    "        dfeat_map_bn = relugradient(dfeat_map_act,feat_map_act)\n",
    "        dfeat_map,dconv1gam,dconv1bet = convbatchnorm_backward(dfeat_map_bn,conv1_cache)\n",
    "        #print(dconv1gam.shape)\n",
    "        dfilt1,dX = conv_backward(dfeat_map,myfilter1,temp)\n",
    "        \n",
    "        gradients =(dfilt1,dfilt2,dW1,dW2,db2,dconv1gam,dconv2gam,dconv1bet,dconv2bet,dgam,dbet)\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_optimizer(parameters,gradients,alpha):\n",
    "        (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)=parameters\n",
    "        (dfilt1,dfilt2,dW1,dW2,db2,dconv1gam,dconv2gam,dconv1bet,dconv2bet,dgam,dbet)=gradients\n",
    "        myfilter1 = myfilter1 -alpha*dfilt1\n",
    "        myfilter2 = myfilter2 -alpha*dfilt2\n",
    "        W1 = W1 - alpha * dW1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "        conv1gam = conv1gam -alpha * dconv1gam\n",
    "        conv1bet = conv1bet -alpha * dconv1bet\n",
    "        conv2gam = conv2gam -alpha * dconv2gam\n",
    "        conv2bet = conv2bet -alpha * dconv2bet        \n",
    "        gam = gam - alpha * dgam\n",
    "        bet = bet - alpha * dbet\n",
    "        updated_parameters = (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)\n",
    "        return updated_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_CNN(temp,ytemp,temp1,ytemp1,myfilter1,myfilter2,fc1_neurons,num_class,alpha=0.05,iterations=50):\n",
    "    Loss1=[]\n",
    "    Accuracy1=[]\n",
    "    Loss2=[]\n",
    "    Accuracy2=[]\n",
    "    alpha=alpha\n",
    "    iterations=iterations\n",
    "    gam=1\n",
    "    bet=0\n",
    "    conv1gam=np.ones(myfilter1.shape[3]).reshape(1,1,myfilter1.shape[3],1)\n",
    "    conv2gam=np.ones(myfilter2.shape[3]).reshape(1,1,myfilter2.shape[3],1)\n",
    "    conv1bet=np.zeros(myfilter1.shape[3]).reshape(1,1,myfilter1.shape[3],1)\n",
    "    conv2bet=np.zeros(myfilter2.shape[3]).reshape(1,1,myfilter2.shape[3],1)\n",
    "    size=tell_size(temp,ytemp,myfilter1,myfilter2)\n",
    "    W1 = np.random.normal(0,0.1,(size,fc1_neurons))\n",
    "    W2 = np.random.normal(0,0.1,(fc1_neurons,num_class))\n",
    "    b2 = np.zeros((1,num_class))       \n",
    "    parameters = (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)\n",
    "    for i in range(iterations):\n",
    "        loss1,accuracy1,cache_train=forward_propagate(temp,ytemp,parameters,fc1_neurons,num_class)\n",
    "        loss2,accuracy2,cache_test=forward_propagate(temp1,ytemp1,parameters,fc1_neurons,num_class)\n",
    "        \n",
    "        Loss1.append(loss1)\n",
    "        Accuracy1.append(accuracy1)\n",
    "        Loss2.append(loss2)\n",
    "        Accuracy2.append(accuracy2)\n",
    "        \n",
    "        print(str(i)+\":Train Loss:\"+str(loss1)+\" | Train Accuracy: \"+str(accuracy1))\n",
    "        print(str(i)+\":Test Loss:\"+str(loss2)+\" | Test Accuracy: \"+str(accuracy2))\n",
    "        \n",
    "        gradients=backward_propagate(temp,ytemp,cache_train)\n",
    "        parameters=gradient_descent_optimizer(parameters,gradients,alpha)\n",
    "    return Loss1,Accuracy1,Loss2,Accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Loss:2.3955485204261833 Accuracy: 0.058\n",
      "0:Test Loss:2.376046717953194 | Test Accuracy: 0.073\n",
      "1: Loss:2.218020610898184 Accuracy: 0.177\n",
      "1:Test Loss:2.2437489471572722 | Test Accuracy: 0.139\n",
      "2: Loss:2.0853944244241904 Accuracy: 0.355\n",
      "2:Test Loss:2.1420160959253978 | Test Accuracy: 0.256\n",
      "3: Loss:1.9685825035794149 Accuracy: 0.485\n",
      "3:Test Loss:2.0487929265749547 | Test Accuracy: 0.371\n",
      "4: Loss:1.853142717533198 Accuracy: 0.569\n",
      "4:Test Loss:1.954309750412736 | Test Accuracy: 0.453\n",
      "5: Loss:1.7346929500987083 Accuracy: 0.627\n",
      "5:Test Loss:1.8556369715406418 | Test Accuracy: 0.504\n",
      "6: Loss:1.6131935688832308 Accuracy: 0.657\n",
      "6:Test Loss:1.7529607317409532 | Test Accuracy: 0.538\n",
      "7: Loss:1.4909081595719682 Accuracy: 0.696\n",
      "7:Test Loss:1.6471526653589676 | Test Accuracy: 0.561\n",
      "8: Loss:1.3717372654291187 Accuracy: 0.729\n",
      "8:Test Loss:1.5419635825689355 | Test Accuracy: 0.593\n",
      "9: Loss:1.2587098365548828 Accuracy: 0.764\n",
      "9:Test Loss:1.4379935959259904 | Test Accuracy: 0.623\n",
      "10: Loss:1.1534356334596598 Accuracy: 0.781\n",
      "10:Test Loss:1.339854378295472 | Test Accuracy: 0.651\n",
      "11: Loss:1.0564028620600947 Accuracy: 0.805\n",
      "11:Test Loss:1.2479247848308894 | Test Accuracy: 0.681\n",
      "12: Loss:0.9675585337694701 Accuracy: 0.821\n",
      "12:Test Loss:1.1634909948972803 | Test Accuracy: 0.704\n",
      "13: Loss:0.88651601953631 Accuracy: 0.836\n",
      "13:Test Loss:1.086770110813119 | Test Accuracy: 0.723\n",
      "14: Loss:0.8129480107749661 Accuracy: 0.855\n",
      "14:Test Loss:1.0167262686065608 | Test Accuracy: 0.746\n",
      "15: Loss:0.746119874175138 Accuracy: 0.871\n",
      "15:Test Loss:0.9530750430667708 | Test Accuracy: 0.764\n",
      "16: Loss:0.685986040568275 Accuracy: 0.884\n",
      "16:Test Loss:0.8954203710341911 | Test Accuracy: 0.772\n",
      "17: Loss:0.6315283951497302 Accuracy: 0.893\n",
      "17:Test Loss:0.8433914865926722 | Test Accuracy: 0.784\n",
      "18: Loss:0.5826605161624682 Accuracy: 0.901\n",
      "18:Test Loss:0.7977700742378833 | Test Accuracy: 0.794\n",
      "19: Loss:0.5384519155899125 Accuracy: 0.908\n",
      "19:Test Loss:0.7558031761362224 | Test Accuracy: 0.804\n",
      "20: Loss:0.4988859204330116 Accuracy: 0.913\n",
      "20:Test Loss:0.7188244215386248 | Test Accuracy: 0.817\n",
      "21: Loss:0.46332118493421354 Accuracy: 0.923\n",
      "21:Test Loss:0.6844206095188909 | Test Accuracy: 0.821\n",
      "22: Loss:0.4311491818072732 Accuracy: 0.926\n",
      "22:Test Loss:0.6545311176580536 | Test Accuracy: 0.83\n",
      "23: Loss:0.40195766972445807 Accuracy: 0.93\n",
      "23:Test Loss:0.6269318687903611 | Test Accuracy: 0.838\n",
      "24: Loss:0.3755988608726301 Accuracy: 0.935\n",
      "24:Test Loss:0.6025358323979837 | Test Accuracy: 0.843\n",
      "25: Loss:0.3516878755334206 Accuracy: 0.945\n",
      "25:Test Loss:0.5804065239406389 | Test Accuracy: 0.846\n",
      "26: Loss:0.330046491975729 Accuracy: 0.947\n",
      "26:Test Loss:0.5604270960272543 | Test Accuracy: 0.85\n",
      "27: Loss:0.31026090393770667 Accuracy: 0.95\n",
      "27:Test Loss:0.5422349539105934 | Test Accuracy: 0.855\n",
      "28: Loss:0.29191654386670124 Accuracy: 0.953\n",
      "28:Test Loss:0.5250450494342856 | Test Accuracy: 0.857\n",
      "29: Loss:0.2752881085316343 Accuracy: 0.957\n",
      "29:Test Loss:0.509748324485886 | Test Accuracy: 0.86\n",
      "30: Loss:0.2600071249843519 Accuracy: 0.959\n",
      "30:Test Loss:0.4955464549065703 | Test Accuracy: 0.862\n",
      "31: Loss:0.24590700959495812 Accuracy: 0.962\n",
      "31:Test Loss:0.48295242707527875 | Test Accuracy: 0.862\n",
      "32: Loss:0.23266625043833425 Accuracy: 0.964\n",
      "32:Test Loss:0.47056557845930275 | Test Accuracy: 0.865\n",
      "33: Loss:0.2203516481896621 Accuracy: 0.967\n",
      "33:Test Loss:0.4600620844179501 | Test Accuracy: 0.869\n",
      "34: Loss:0.2088984157865983 Accuracy: 0.969\n",
      "34:Test Loss:0.4501209147251809 | Test Accuracy: 0.871\n",
      "35: Loss:0.1982724513689723 Accuracy: 0.972\n",
      "35:Test Loss:0.4411557024379833 | Test Accuracy: 0.875\n",
      "36: Loss:0.18840594454746104 Accuracy: 0.975\n",
      "36:Test Loss:0.43295161089194106 | Test Accuracy: 0.877\n",
      "37: Loss:0.17925422068276461 Accuracy: 0.977\n",
      "37:Test Loss:0.4251497893978493 | Test Accuracy: 0.878\n",
      "38: Loss:0.17072189651017217 Accuracy: 0.979\n",
      "38:Test Loss:0.4189078049410421 | Test Accuracy: 0.88\n",
      "39: Loss:0.16275442741678267 Accuracy: 0.98\n",
      "39:Test Loss:0.4119451612635793 | Test Accuracy: 0.881\n",
      "40: Loss:0.1552060653286434 Accuracy: 0.98\n",
      "40:Test Loss:0.4065089491751909 | Test Accuracy: 0.884\n",
      "41: Loss:0.14817837957157018 Accuracy: 0.982\n",
      "41:Test Loss:0.40097126213428796 | Test Accuracy: 0.888\n",
      "42: Loss:0.1414670931559284 Accuracy: 0.983\n",
      "42:Test Loss:0.39620683599197054 | Test Accuracy: 0.89\n",
      "43: Loss:0.1352527363868229 Accuracy: 0.985\n",
      "43:Test Loss:0.3919274252187846 | Test Accuracy: 0.889\n",
      "44: Loss:0.12938786244126177 Accuracy: 0.985\n",
      "44:Test Loss:0.3875043052281449 | Test Accuracy: 0.889\n",
      "45: Loss:0.12385963616052711 Accuracy: 0.987\n",
      "45:Test Loss:0.3839813957211645 | Test Accuracy: 0.889\n",
      "46: Loss:0.11866099303569279 Accuracy: 0.988\n",
      "46:Test Loss:0.3800732959873048 | Test Accuracy: 0.888\n",
      "47: Loss:0.11374091486938041 Accuracy: 0.989\n",
      "47:Test Loss:0.3770222992089155 | Test Accuracy: 0.889\n",
      "48: Loss:0.1090869589064645 Accuracy: 0.99\n",
      "48:Test Loss:0.37347365038485464 | Test Accuracy: 0.891\n",
      "49: Loss:0.10469510005546168 Accuracy: 0.99\n",
      "49:Test Loss:0.3711467801204265 | Test Accuracy: 0.894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x93808f8400>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfilter1 = np.random.normal(0,0.1,9*16).reshape(3,3,1,16)\n",
    "myfilter2 = np.random.normal(0,0.1,16*16*18).reshape(4,4,16,18)\n",
    "\n",
    "trainlossbn,trainaccuracybn,testlossbn,testaccuracybn=train_CNN(temp,ytemp,temp1,ytemp1,myfilter1,myfilter2,fc1_neurons=20,num_class=10,alpha=0.5,iterations=50)    \n",
    "pd.Series(trainlossbn).plot()    \n",
    "pd.Series(trainaccuracybn).plot()\n",
    "pd.Series(testlossbn).plot()    \n",
    "pd.Series(testaccuracybn).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
